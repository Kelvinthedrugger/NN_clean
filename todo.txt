
implement paper:
  super-convergence: cycle learning rate


perform experiments on implement dataset:
  Mnist, Cifar's'

for model.save() method
  1. go down very deep (from scratch)
  2. use pickle library


Layers:
  
  LSTM

  Conv layer
    __call__() for child/gradient pass
    forward()
    backward()

    fix stride problem
    fix number of filters problem
    fix padding problem


 open a folder named "trees" & rewrite neural engine based on tree
traversal:
 
 structure:
       root node: loss function(s)
       leaf node: input layer(s)
    forward pass: postorder 
        backprop: preorder

 adopt *args, **kwargs

